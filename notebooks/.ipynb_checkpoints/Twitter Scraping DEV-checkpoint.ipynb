{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "26109bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e2e34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "345e72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing adding the \"tunable\" parameters into this cell\n",
    "\n",
    "KEY_PHRASES = [\"climate change\", \"sustainability\", \"technology\"]\n",
    "START_DATE = \"2021-01-01\"\n",
    "END_DATE = \"2021-08-30\"\n",
    "MAX_TWEETS = 2000\n",
    "\n",
    "MIN_FOLLOWERS = 5000\n",
    "MAX_TWEETS_PER_USER = 200\n",
    "\n",
    "NUMBER_OF_TOPICS = 3\n",
    "NO_TOP_WORDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [item for words in KEY_PHRASES for item in words.split(' ')]\n",
    "key_word_sets = [item for word in key_words for item in wordnet.synsets(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb04ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list to append tweet data to\n",
    "tweet_list = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for key_phrase in KEY_PHRASES:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\n",
    "                        key_phrase + ' since:' + START_DATE + ' until:' + END_DATE).get_items()):\n",
    "        if i > MAX_TWEETS:\n",
    "            break\n",
    "        tweet_list.append([tweet.user.username, tweet.user.followersCount, tweet.user.verified])\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweet_df = pd.DataFrame(tweet_list, columns=['Username', 'FollowerCount', 'Verified'])\n",
    "tweet_df = tweet_df.loc[tweet_df['FollowerCount'] >= MIN_FOLLOWERS]\n",
    "tweet_df = tweet_df.drop_duplicates(subset=['Username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e9b189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@'\n",
    "\n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "# cleaning master function\n",
    "def clean_tweet(tweet, bigrams=False):\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    tweet_token_list = [word for word in tweet.split(' ')\n",
    "                            if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in tweet_token_list] # apply word rooter\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d13ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b2f0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_usernames = tweet_df['Username'].values\n",
    "user_max_scores = []\n",
    "user_avg_scores = []\n",
    "user_LDA_model = []\n",
    "\n",
    "for twitter_user in candidate_usernames:\n",
    "    \n",
    "    # Creating list to append tweet data to\n",
    "    user_tweets = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + twitter_user).get_items()):\n",
    "        if i > MAX_TWEETS_PER_USER:\n",
    "            break\n",
    "        user_tweets.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "    # Creating a dataframe from the tweets list above \n",
    "    user_tweets_df = pd.DataFrame(user_tweets, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "\n",
    "    documents = list(user_tweets_df['Text'].values)\n",
    "    cleaned_document = [clean_tweet(tweet) for tweet in documents]\n",
    "    cleaned_document = [cleaned_tweet for cleaned_tweet in cleaned_document if cleaned_tweet != ' ']\n",
    "    \n",
    "    # the vectorizer object will be used to transform text to vector form\n",
    "    vectorizer = CountVectorizer(token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "    # apply transformation\n",
    "    tf = vectorizer.fit_transform(cleaned_document).toarray()\n",
    "\n",
    "    # tf_feature_names tells us what word each column in the matric represents\n",
    "    tf_feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    model = LatentDirichletAllocation(n_components=NUMBER_OF_TOPICS, random_state=0)\n",
    "    model.fit(tf)\n",
    "    \n",
    "    topic_df = display_topics(model, tf_feature_names, NO_TOP_WORDS)\n",
    "    \n",
    "    user_words = []\n",
    "    for topic_number in range(NUMBER_OF_TOPICS):\n",
    "        topic_words = topic_df[\"Topic \" + str(topic_number) + \" words\"].values\n",
    "        user_words += [word for word in topic_words]\n",
    "        \n",
    "    user_word_sets = [item for word in user_words for item in wordnet.synsets(word)]\n",
    "    \n",
    "    max_val = 0\n",
    "    avg_val = 0\n",
    "    count = 0\n",
    "    #max_sets = ()\n",
    "    for set_1, set_2 in product(key_word_sets, user_word_sets):\n",
    "        similarity = set_1.wup_similarity(set_2)\n",
    "        avg_val += similarity\n",
    "        count += 1\n",
    "        if similarity > max_val:\n",
    "            max_val = similarity\n",
    "            #max_sets = (set_1, set_2)\n",
    "    avg_val /= count\n",
    "    \n",
    "    user_max_scores.append(max_val)\n",
    "    user_avg_scores.append(avg_val)\n",
    "    user_LDA_model.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61460d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6147e3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_components', 'doc_topic_prior', 'topic_word_prior', 'learning_method', 'learning_decay', 'learning_offset', 'max_iter', 'batch_size', 'evaluate_every', 'total_samples', 'perp_tol', 'mean_change_tol', 'max_doc_update_iter', 'n_jobs', 'verbose', 'random_state', 'n_features_in_', 'random_state_', 'n_batch_iter_', 'n_iter_', 'doc_topic_prior_', 'topic_word_prior_', 'components_', 'exp_dirichlet_component_', 'bound_'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f1f1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['username', 'id', 'displayname', 'description', 'rawDescription', 'descriptionUrls', 'verified', 'created', 'followersCount', 'friendsCount', 'statusesCount', 'favouritesCount', 'listedCount', 'mediaCount', 'location', 'protected', 'linkUrl', 'linkTcourl', 'profileImageUrl', 'profileBannerUrl', 'label'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[0].user.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
